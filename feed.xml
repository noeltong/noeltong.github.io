<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://noeltong.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://noeltong.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-02-13T03:23:55+00:00</updated><id>https://noeltong.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">2D Fan-Beam Tomography</title><link href="https://noeltong.github.io/blog/2024/2d-fan-beam-tomography/" rel="alternate" type="text/html" title="2D Fan-Beam Tomography"/><published>2024-10-17T16:36:16+00:00</published><updated>2024-10-17T16:36:16+00:00</updated><id>https://noeltong.github.io/blog/2024/2d-fan-beam-tomography</id><content type="html" xml:base="https://noeltong.github.io/blog/2024/2d-fan-beam-tomography/"><![CDATA[<h2 id="fan-parallel-rebinning-methods">Fan-parallel rebinning methods</h2> <p>to be finished..</p> <h2 id="the-filter-backproject-fbp-approach-for-tomographic-scan">The filter-backproject (FBP) approach for tomographic scan</h2> <p>The fan-beam FBP method uses the following three steps.</p> <ol> <li>Compute <em>weighted</em> projections for each $\beta$. \begin{equation} \tilde{p} (s, \beta) \triangleq p(s, \beta) \frac{w_{2\pi} (s, \beta) J(s)}{W_1 ^2 (s)}. \end{equation}</li> <li>Filter those weighted projections (along $s$) for each $\beta$ using the modified ramp filter \begin{equation} \check{p} (s, \beta) \triangleq \tilde{p} (s, \beta) * g_{*} (s),\ \forall \beta. \end{equation}</li> <li>Perform a weighted backprojection of those filtered projections: \begin{equation} f(x, y) = \int_0 ^{2\pi} \frac{1}{W_2^2 (x, y, \beta) L_{\beta} ^2 (x, y)} \check{p} (s_{\beta} (x, y), \beta) \mathrm{d} \beta. \end{equation}</li> </ol>]]></content><author><name></name></author><category term="lecture-notes"/><category term="inverse"/><summary type="html"><![CDATA[Notes on backprojection algorithm of fan-beam CT]]></summary></entry><entry><title type="html">Review on Implicit Neural Representations and its Applications in Medical Imaging</title><link href="https://noeltong.github.io/blog/2024/nerf/" rel="alternate" type="text/html" title="Review on Implicit Neural Representations and its Applications in Medical Imaging"/><published>2024-10-17T16:36:16+00:00</published><updated>2024-10-17T16:36:16+00:00</updated><id>https://noeltong.github.io/blog/2024/nerf</id><content type="html" xml:base="https://noeltong.github.io/blog/2024/nerf/"><![CDATA[<h2 id="implicit-neural-representation-inr">Implicit Neural Representation (INR)</h2> <ul> <li>Explicit representation <ul> <li>Conventional approaches to encoding input signals as representations typically follow an explicit paradigm, where the input space is discretized or partitioned into separate elements (e.g., point clouds, voxel grids, and meshes).</li> <li>Explicit (or discrete) representations directly encode the features or signal values.</li> </ul> </li> <li>Implicit neural representation (INR) <ul> <li>Implicit representations are defined as a generator function that maps input coordinates to their corresponding value within the input space.</li> <li>A Multi-Layer Perceptron (MLP) is trained to parameterize the signal of interest, such as an image or shape, utilizing coordinates as input.</li> <li>The objective is to predict the corresponding data values at those coordinates. Thus, the MLP serves as an Implicit Neural Representation function that encodes the signalâ€™s representation within its weights.</li> </ul> </li> </ul> <p>For INR, a simple MLP can be learned to continuously represent the signal of interest as an implicit function \begin{equation} \Psi: \mathbf{x} \in \mathbb{R}^M \to \Psi (\mathbf{x}) \in \mathbb{R}^N, \end{equation} mapping their \(M\)-dimensinal spatial coordinates to \(N\)-dimensional values (e.g., color, occupancy, etc.).</p> <h3 id="input">Input</h3> <ul> <li>The conventional approach in INR treats the spatial coordinate of each element in the signal, such as pixels in an image, as the input to an MLP. However, this approach tends to learn low-frequency functions, limiting its ability to effectively represent complex signals.</li> <li>Using a sinusoidal mapping of the Cartesian coordinates to a higher dimensional space, which enables the learning of high-frequency details more effectively. Denote the signal coordinates as \(\mathbf{v}\), we have different ways of incorporate positional information, <ul> <li>Basic: \(\gamma (\mathbf{v}) = \left[ \cos 2\pi\mathbf{v}, \sin 2\pi\mathbf{v} \right]^\top\).</li> <li>Positional encoding (PE): \(\gamma (\mathbf{v}) = \left[ \cdots, \cos 2\pi\sigma^{j/m}\mathbf{v}, \sin 2\pi\sigma^{j/m}\mathbf{v}, \cdots \right]^\top\) for \(j = 0, \cdots, m-1\). The hyperparameter \(\sigma\) is selected considering different tasks and datasets.</li> <li>Gaussian PE: \(\gamma (\mathbf{v}) = \left[ \cos 2\pi\mB\mathbf{v}, \sin 2\pi\mB\mathbf{v} \right]^\top\), where \(\mB\) is a matrix whose elements are sampled from a Gaussian distribution \(\gN \left( 0, \sigma^2 \right)\). The scale \(\sigma\) is selected through a hyperparameter sweep for each task and dataset.</li> </ul> </li> </ul> <h3 id="activations">Activations</h3> <ul> <li>For implicit representations, nonlinearities can be either periodic or nonperiodic. However, non-periodic functions, such as \texttt{ReLU} or \texttt{tanh}, are not conducive to the effective learning of highfrequency signals.</li> <li>To address this issue, sine functions can be utilized as the activation function of the MLP to parametrize complex data. \begin{equation} \Psi (\mathbf{x}) = \mathbf{W}<em>n \left(\psi</em>{n-1} \circ \psi_{n-2} \circ \cdots \circ \psi_0 \right) (\mathbf{x}) + \vb_n \end{equation} \begin{equation} \mathbf{x}<em>i \mapsto \psi_i (\mathbf{x}</em>{i}) = \sin \left( \mathbf{W}_i \mathbf{x}_i + \vb_i \right). \end{equation} where \(\psi_i\) indicates the \(i\)-th layer of the MLP, \(\mathbf{W}_i\), \(\vb_i\) is the corresponding weight and bias, and \(\mathbf{x}\) is the signal of interest.</li> </ul> <h3 id="neural-radiance-field">Neural Radiance Field</h3> <ul> <li>Neural Radiance Fields (NeRFs) unites INRs with volume rendering by using fully connected MLPs to implicitly represent scenes and objects with the goal of novel view synthesis. The objective of novel view synthesis is to develop a system that can generate novel viewpoints of an object from any direction by observing a few images of that particular object. The process is defined as \begin{equation} F(\mathbf{x}, \mathbf{d}) \to (\mathbf{c}, \sigma). \end{equation} where \(\mathbf{x}\) indicates the 3D location \((x, y, z)\), \(d\) represents the 2D vector of viewing direction \((\theta, \phi)\), \(\mathbf{c}\) denotes the RGB color values \((R, G, B)\), and \(\sigma\) is the voxel intensity.</li> <li>Original NeRF uses ReLU activation and a positional encoding approach to map coordinates to higher dimensions, \begin{equation} \gamma (p) = \left( \sin 2^0 \pi p, \cos 2^0 \pi p, \cdots, \sin 2^{L-1} \pi p, \cos 2^{L-1} \pi p \right), \end{equation} where \(p\) can be each of the coordinate or viewing direction components.</li> <li>NeRF follows a two-stage procedure to obtain voxel density and color values, <ul> <li>\(\sigma, h = \text{MLP} (\mathbf{x})\).</li> <li>\(c = \text{MLP} ([h, d])\).</li> </ul> </li> </ul> <h2 id="review-of-inr-in-medical-imaging">Review of INR in Medical Imaging</h2> <h3 id="medical-image-reconstruction">Medical Image Reconstruction</h3> <h3 id="neural-rendering">Neural Rendering</h3>]]></content><author><name></name></author><category term="paper-review"/><category term="inverse"/><summary type="html"><![CDATA[review on inr]]></summary></entry></feed>